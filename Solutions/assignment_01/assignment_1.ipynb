{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TPM034A Machine Learning for socio-technical systems \n",
    "## `Assignment 01: Data Exploration and MultiLayer Perceptrons`\n",
    "\n",
    "**Delft University of Technology**<br>\n",
    "**Q2 2024**<br>\n",
    "**Instructor:** Sander van Cranenburgh <br>\n",
    "**TAs:**  Francisco Garrido Valenzuela & Lucas Spierenburg <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Instructions`\n",
    "\n",
    "**Assignments aim to:**<br>\n",
    "* Examine your understanding of the key concepts and techniques.\n",
    "* Examine your the applied ML skills.\n",
    "\n",
    "**Assignments:**<br>\n",
    "* Are graded and must be submitted (see the submission instruction below). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Google Colab workspace set-up`\n",
    "\n",
    "Uncomment the following cells code lines if you are running this notebook on Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/TPM034A/Q2_2024\n",
    "#!pip install -r Q2_2024/requirements_colab.txt\n",
    "#!mv \"/content/Q2_2024/Assignments/assignment_01/data\" /content/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Application: Cycling speed prediction for Rotterdam` <br>\n",
    "\n",
    "### **Introduction**\n",
    "\n",
    "In Dutch urban context, cycling is an important mode of transportation, serving both personal and commercial purposes, including delivery services. However, one of the challenges that individuals and companies frequently encounter is the lack of relevant cycling itineraries proposed by routing algorithms. This is partly due to the lack of accurate cycling speed information per road link. Hence, accurate information on cycling speeds could improve itinerary recommendation by routing apps, and, in turn, help individuals and companies to choose better cycling routes. \n",
    "\n",
    "One way to obtain accurate cypling speed data is by installing speed sensors on all road links. However, this is a costly and time-consuming process. Alternatively, one could use machine learning models to estimate cycling speeds based on other variables, such as road infrastructure, traffic, and weather conditions.\n",
    "\n",
    "Seeing a business opportunity, a data-analytics start-up company has collected data on average cycling speeds on several roads within a specific neighborhood. Now it needs to develop a machine learning model that can predict cycling speeds on any road link in the city of Rotterdam. You have been hired by this company for this task. Specifically, in this assignment you need todevelop an MLP that is capable of predicting the (average) cycling speeds per road link for the entire city of Amsterdam based publicly available street infrastructure data.\n",
    "\n",
    "List of tasks:\n",
    "1. Explore the cycling speed data provided by the company, to determine if it can be used for your task\n",
    "1. Train an MLP model to predict average cycling speed\n",
    "1. Evaluate and reflect on the performance of your model\n",
    "\n",
    "### **Data**\n",
    "\n",
    "You have access to 2 data sets:\n",
    "1. Training dataset: first_campaign.gpkg\n",
    "1. Testing dataset: validation_campaign.gpkg\n",
    "<br>\n",
    "\n",
    "### **Tasks and grading**\n",
    "\n",
    "Your assignment is divided into 4 subtasks: (1) data inspection, (2) regression model, (3) MLP model, (4) performing an out-of-sample validation.\n",
    "\n",
    "1.  **Data inspection: Load the dataset and make a first inspection** [1.5 pnt]\n",
    "    1. Distribution of cycling speed:\n",
    "        1. Visualize the statistical distribution of cycling speed per street segment\n",
    "        1. Plot its spatial distribution on a map. Do the data make sense?\n",
    "    1. Number of observations per street:\n",
    "        1. Plot the distribution of number of observations per street\n",
    "        1. How many street segments have less than 3 observations?  Why is it a problem?\n",
    "1. **Training a first regression model** [3 pnt]\n",
    "    1. Selecting relevant columns: Which columns should you keep in your analysis? Why? \n",
    "    1. One-hot encoding: Which variables would you need to encode and why? (provide a list)\n",
    "    1. Split the data between training and test set.\n",
    "    1. Start with a simple model:\n",
    "        - Use a linear regression model\n",
    "        - Make that the columns are in the same order in both train and test sets.\n",
    "        - Evaluate the performance on the test set, using the R2, is the performance decent to use this model?\n",
    "    1. Plot the predicted speed on the test set against the true speed. Is there a speed region where the prediction are better/poorer?\n",
    "    1. Print the coefficient of the regression. Which features contribute to faster speed? Does that make sense?\n",
    "1. **Training a more advanced MLP model** [2.5 pnt]\n",
    "    1. Scaling variable: Which variables would you scale and why? (provide a list), use a minmax scaler.\n",
    "    1. Hyperparameter tuning: Design a grid search over the following hyperparameter space:\n",
    "        - hidden_layer_sizes: [(18),(10,10,10,10,10),(18,10)]\n",
    "        - learning_rate_init: [1e-2,1e-3,1e-5]\n",
    "        - alpha: [1,0.1]\n",
    "    1. Evaluate the performance on the test set, using the R2. How is the performance compared to the previous regression model?\n",
    "    1. Plot the predicted speed on the test set against the true speed. Is there a speed region where the prediction are better/poorer?\n",
    "1. **Out-of-sample validation** [3 pnt]\n",
    "    The company that hired you is impressed by your results. But to get more confidence about the generalisation performance, they decided to collect a new data set in another neighborhood.\n",
    "    1. Load and preprocess the validation data.\n",
    "        - Make sure that the validation data have the same columns as the original data in the same order\n",
    "        - Apply the same preprocessing as the data used for training\n",
    "    1. Measure the generalisation performance of the MLP model on the hold-out sample data.\n",
    "        1. Measure the performance\n",
    "        1. Plot the model's prediction against the true speed\n",
    "    1. Measure the generalisation performance of the regression model on the hold-out sample data\n",
    "        1. Measure the performance\n",
    "        1. Plot the model's prediction against the true speed\n",
    "    1. Reflect on the generalisation performance of your model\n",
    "        1. Discuss reasons why the models might perform better or worse on the validation data.\n",
    "        1. Which model performs better? Why?\n",
    "\n",
    "\n",
    "### **Submission**\n",
    "- The deadline for this assignment is **Monday, XX XXXXXX XXXX** \n",
    "- Use **Python 3.XX or above**\n",
    "- You have to submit your work in zip file with the ipynb **(fully executed)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data inspection: Load the dataset and make a first inspection\n",
    "#### 1.1 Distribution of cycling speed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "data_path = Path('data')\n",
    "cycling_links = gpd.read_file(data_path / 'first_campaign.gpkg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 Visualize the statistical distribution of cycling speed per street segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the statistical distribution of cycling speed per street segment\n",
    "cycling_links['speed'].hist(bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 Plot its spatial distribution on a map. Do the data make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot its spatial distribution on a map. Do the data make sense?\n",
    "cycling_links.explore(column='speed')\n",
    "\n",
    "# Answer: Yes, the data makes sense. Speed is higher along main roads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Number of observation per street"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Plot the distribution of number of observations per street"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycling_links['n_observations'].hist(bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 How many street segments have two or less observations? Why is it a problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cycling_links.loc[cycling_links['n_observations']<3])\n",
    "# Answer: Many street segments have less than 3 observations, which threatens reliable predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Training multiple linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Selecting relevant columns: Which columns should you keep in your analysis? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycling_links.columns\n",
    "#Keep all columns but 'osmid' (no relevant information), 'geometry' and 'n_observations' for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_keep = cycling_links.columns.drop(['n_observations','osmid', 'geometry'])\n",
    "cycling_links = cycling_links[cols_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 One-hot encoding: Which variables would you need to encode and why? (provide a list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical features\n",
    "features_categorical = ['oneway', 'highway', 'bridge', 'service', 'access', 'tunnel', 'junction','street_light'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycling_links[features_categorical] = cycling_links[features_categorical].astype('category')\n",
    "cycling_links[features_categorical].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in features_categorical:\n",
    "    print(f'FEATURE = {feature}\\n')\n",
    "    print(cycling_links[feature].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hot encode categorical features\n",
    "df = pd.get_dummies(cycling_links, columns=features_categorical, drop_first=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Split the data into a training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the DataFrame into train and test sets\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "features = df.columns.tolist()\n",
    "features.remove('speed')\n",
    "features\n",
    "\n",
    "X_train = df_train[features]\n",
    "y_train = df_train['speed']\n",
    "X_test = df_test[features]\n",
    "y_test = df_test['speed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Train your first model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a linear regression model\n",
    "# Make that the columns are in the same order in both train and test sets.\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance on the test set, using the R2\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R² Score: {r2}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Plot the predicted speed on the test set against the true speed. Is there a speed region where the prediction are better/poorer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of the true values vs the predicted values using sns_regplot\n",
    "sns.regplot(x=df_test['speed'], y=y_pred, scatter_kws={'alpha':0.3})\n",
    "plt.xlim(5, 35)\n",
    "plt.ylim(5, 35)\n",
    "plt.xlabel('True values')\n",
    "plt.ylabel('Predicted values')\n",
    "plt.title('True vs Predicted values')\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.show()\n",
    "# Outliers are located in the extremes of the speed range, and upper outliers make the model overestimate the speed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6 Print the coefficient of the regression. Which features contribute to faster speed? Does that make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the coefficients with their corresponding feature names\n",
    "coefficients = pd.DataFrame({'feature': features, 'coefficient': model.coef_})\n",
    "\n",
    "# Add the intercept to the DataFrame\n",
    "intercept_df = pd.DataFrame({'feature': ['intercept'], 'coefficient': [model.intercept_]})\n",
    "coefficients = pd.concat([intercept_df,coefficients], ignore_index=True)\n",
    "\n",
    "coefficients.sort_values('coefficient', ascending=False)\n",
    "\n",
    "# Larger roads (primary, secondary) have a high positive impact on speed.\n",
    "# Street light have negative impact on speed.\n",
    "# Residential roads have a negative impact on speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Train a more advanced model: MLP\n",
    "hint: \n",
    "- When feeding tabular data to sklearn use a pandas dataframe instead of a numpy array \n",
    "- It allows sklearn to control for the order of the columns\n",
    "- It will be useful later in the assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Scaling numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_numeric = ['length','lanes','maxspeed']\n",
    "\n",
    "df_train_scaled = df_train.copy()\n",
    "df_test_scaled = df_test.copy()\n",
    "# Scaling numerical features\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_numeric = X_train[features_numeric]\n",
    "X_train_cat = X_train.drop(columns=features_numeric)\n",
    "X_test_numeric = X_test[features_numeric]\n",
    "X_test_cat = X_test.drop(columns=features_numeric)\n",
    "\n",
    "# Fit the scaler on the training data\n",
    "scaler.fit(X_train_numeric)\n",
    "\n",
    "X_train_numeric_scaled = scaler.transform(X_train_numeric)\n",
    "X_test_numeric_scaled = scaler.transform(X_test_numeric)\n",
    "\n",
    "# Scaling the training and test data\n",
    "df_X_train_scaled_numeric = pd.DataFrame(X_train_numeric_scaled, columns=features_numeric)\n",
    "df_X_train_scaled = pd.concat([df_X_train_scaled_numeric, X_train_cat.reset_index(drop=True)], axis=1)\n",
    "X_test_scaled_numeric_df = pd.DataFrame(X_test_numeric_scaled, columns=features_numeric)\n",
    "df_X_test_scaled = pd.concat([X_test_scaled_numeric_df, X_test_cat.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Hyperparameter tuning: Design a grid search over the following hyperparameter space:\n",
    " - hidden_layer_sizes: [(18),(10,10,10,10,10),(18,10)]\n",
    " - learning_rate_init: [1e-2,1e-3,1e-5]\n",
    " - alpha: [1,0.1]\n",
    "\n",
    "Fixed hyperparameters:\n",
    " - activation = 'relu'\n",
    " - solver = 'adam'\n",
    " - batch_size = 50\n",
    " - max_iter = 2000\n",
    " - random_state = 42\n",
    " \n",
    " Parameter for the grid search:\n",
    " - cv = 5\n",
    " - scoring = 'r2'\n",
    " - random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_gs = MLPRegressor(activation = 'relu', solver='adam', batch_size=50, max_iter=2000)\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "hyperparameter_space = {\n",
    "    'hidden_layer_sizes': [(18),(10,10,10,10,10),(18,10)],\n",
    "    'alpha': [1,0.1],\n",
    "    'learning_rate_init': [0.01,0.001]\n",
    "    }\n",
    "\n",
    "folds = 5\n",
    "\n",
    "mlp_gridsearch = GridSearchCV(mlp_gs, hyperparameter_space, n_jobs=-1, cv=folds, scoring = 'r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct the gridsearch (just once)\n",
    "output_folder = Path(data_path/'output')\n",
    "model_sav = 'my_tuned_MLPmodel.sav'\n",
    "if Path.exists(output_folder/model_sav):\n",
    "    mlp_gridsearch = pickle.load(open(output_folder/model_sav,'rb'))\n",
    "else:\n",
    "    if not Path.exists(output_folder):\n",
    "        os.mkdir(output_folder)\n",
    "    \n",
    "    # Conduct the gridsearch\n",
    "    mlp_gridsearch.fit(df_X_train_scaled_numeric, y_train)\n",
    "    \n",
    "    # Save the results\n",
    "    pickle.dump(mlp_gridsearch, open(output_folder/model_sav,'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Train the model with the best parameters and evaluate the performance on the test set, using the R2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gridsearch = pd.DataFrame.from_dict(mlp_gridsearch.cv_results_)\n",
    "# Add new column with a label for the hyperparameter combinations\n",
    "df_gridsearch['gs_combinations'] = 'L2 = '+ df_gridsearch['param_alpha'].astype('str') + '; Learning_rate = '+ df_gridsearch['param_learning_rate_init'].astype('str') + '; Layers = ' + df_gridsearch['param_hidden_layer_sizes'].astype('str')\n",
    "df_gridsearch = df_gridsearch.sort_values('rank_test_score')\n",
    "\n",
    "# Visualise deviation in performance across hyper parameter settings\n",
    "plt.figure(figsize = (16,6))\n",
    "ax = sns.barplot(x = df_gridsearch.gs_combinations, y=df_gridsearch.mean_test_score, palette=\"Blues_d\", hue = df_gridsearch.gs_combinations)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "print('Best hyperparameters found:\\t', mlp_gridsearch.best_params_)\n",
    "print('Best model performance:\\t\\t', mlp_gridsearch.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new mlp object using the optimised hyperparameters, just using the train/test split\n",
    "layers = mlp_gridsearch.best_params_['hidden_layer_sizes']\n",
    "lr = mlp_gridsearch.best_params_['learning_rate_init']\n",
    "alpha = mlp_gridsearch.best_params_['alpha']\n",
    "mlp_gs = MLPRegressor(hidden_layer_sizes = layers, solver='adam', learning_rate_init = lr,\n",
    "                       alpha=alpha, batch_size=50, activation = 'relu', max_iter = 2000) \n",
    "\n",
    "# Train the model\n",
    "mlp_gs.fit(df_X_train_scaled,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance on the test set, using the R2\n",
    "y_pred = mlp_gs.predict(df_X_test_scaled)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(df_test['speed'], y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R² Score: {r2}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Plot the predicted speed on the test set against the true speed. Is there a speed region where the prediction are better/poorer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of the true values vs the predicted values using sns_regplot\n",
    "sns.regplot(x=df_test['speed'], y=y_pred, scatter_kws={'alpha':0.3})\n",
    "plt.xlim(5, 35)\n",
    "plt.ylim(5, 35)\n",
    "plt.xlabel('True values')\n",
    "plt.ylabel('Predicted values')\n",
    "plt.title('True vs Predicted values')\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.show()\n",
    "# Overestimat small values and underestimates large values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Out-of-sample validation\n",
    "The company hiring you finds your result for the models suspiciously good, they decided to collect more data in another neighborhood to check its performance.<br>\n",
    "Performance could change drastically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1. Measure the generalisation performance of the MLP model on the hold-out sample data.\n",
    " - Make sure that the test data have the same columns as the original data in the same order: hint use a pandas dataframe instead of a numpy array (it allows sklearn to control for the order of the columns)\n",
    " - Apply the same preprocessing as the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_of_sample = gpd.read_file(data_path / 'validation_campaign.gpkg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out_of_sample = pd.get_dummies(out_of_sample, columns=features_categorical, drop_first=True)\n",
    "for col in set(df_X_train_scaled.columns) - set(df_out_of_sample.columns):\n",
    "    df_out_of_sample[col] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out_of_sample = df_out_of_sample.reindex(columns=df_X_train_scaled.columns, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out_of_sample_scaled = df_out_of_sample.copy()\n",
    "df_out_of_sample_scaled[features_numeric] = scaler.transform(df_out_of_sample_scaled[features_numeric])\n",
    "# Order the columns in the same way as in the training set\n",
    "df_out_of_sample_scaled = df_out_of_sample_scaled[df_X_train_scaled.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Measure the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_out_of_sample = mlp_gs.predict(df_out_of_sample_scaled)\n",
    "mse_out_of_sample = mean_squared_error(out_of_sample['speed'], y_pred_out_of_sample)\n",
    "r2_out_of_sample = r2_score(out_of_sample['speed'], y_pred_out_of_sample)\n",
    "print(f'Mean Squared Error: {mse_out_of_sample}')\n",
    "print(f'R² Score: {r2_out_of_sample}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 Plot the model's prediction against the true speed. Comment on the model's prediction behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of the true values vs the predicted values using sns_regplot\n",
    "sns.regplot(x=out_of_sample['speed'], y=y_pred_out_of_sample, scatter_kws={'alpha':0.3})\n",
    "plt.xlim(5, 35)\n",
    "plt.ylim(5, 35)\n",
    "plt.xlabel('True values')\n",
    "plt.ylabel('Predicted values')\n",
    "plt.title('True vs Predicted values')\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.show()\n",
    "# Speeds below 20 km/h mapped to around 10 km/h, and speeds above 20 km/h mapped to higher values 25 km/h."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 Measure the generalisation performance of the regression model on the hold-out sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_out_of_sample = model.predict(df_out_of_sample[features])\n",
    "mse_out_of_sample = mean_squared_error(out_of_sample['speed'], y_pred_out_of_sample)\n",
    "r2_out_of_sample = r2_score(out_of_sample['speed'], y_pred_out_of_sample)\n",
    "print(f'Mean Squared Error: {mse_out_of_sample}')\n",
    "print(f'R² Score: {r2_out_of_sample}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 Plot the model's prediction against the true speed. Comment on the model's prediction behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of the true values vs the predicted values using sns_regplot\n",
    "sns.regplot(x=out_of_sample['speed'], y=y_pred_out_of_sample, scatter_kws={'alpha':0.3})\n",
    "plt.xlim(5, 35)\n",
    "plt.ylim(5, 35)\n",
    "plt.xlabel('True values')\n",
    "plt.ylabel('Predicted values')\n",
    "plt.title('True vs Predicted values')\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.show()\n",
    "# Speeds below 20 km/h mapped to around 15 km/h, and speeds above 20 km/h mapped to around 25 km/h."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Interpretation of the results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1 Do you observe a decrease in the performance? If so, what is the cause?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Decrease in performance\n",
    "- Data leak: High spatial correlation between train and validation set (street segments are from the same neighborhood, sometimes the same street)\n",
    "- Overestimation of the performance in the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2 Which of the two models performs better? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Decrease in performance\n",
    "- Data leak: High spatial correlation between train and validation set (street segments are from the same neighborhood, sometimes the same street)\n",
    "- Overestimation of the performance in the validation set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tpm034a24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
