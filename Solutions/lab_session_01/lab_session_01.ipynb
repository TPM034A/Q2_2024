{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TPM034A Machine Learning for socio-technical systems \n",
    "## `Lab session 01: Discover, explore and visualise data`\n",
    "\n",
    "**Delft University of Technology**<br>\n",
    "**Q2 2024**<br>\n",
    "**Instructor:** Sander van Cranenburgh <br>\n",
    "**TAs:**  Francisco Garrido Valenzuela & Lucas Spierenburg <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Instructions`\n",
    "\n",
    "**Lab session aim to:**<br>\n",
    "* Show and reinforce how models and ideas presented in class are put to practice.<br>\n",
    "* Help you gather hands-on machine learning skills.<br>\n",
    "\n",
    "**Lab sessions are:**<br>\n",
    "* Learning environments where you work with Jupyter notebooks and where you can get support from TAs and fellow students.<br> \n",
    "* Not graded and do not have to be submitted. \n",
    "* A good preparation for the **assignments** (which are graded).\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Use of AI tools`\n",
    "AI tools, such as ChatGPT and Copilot, are great tools to assist with programming. Moreover, in your later careers you will work in a world where such tools are widely available. As such, we **encourage** you to use AI tools **effectively** (both in the lab sessions and assignments). However, be careful not to overestimate the capacity of AI tools! AI tools cannot replace you: you still have to conceptualise the problem, dissect it and structure it, to conduct proper analysis and modelling. We recommend being especially **reticent** with using AI tools for the more conceptual and reflective oriented questions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Google Colab workspace set-up`\n",
    "\n",
    "Uncomment the following cells code lines if you are running this notebook on Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/TPM034A/Q2_2024\n",
    "#!pip install -r Q2_2024/requirements_colab.txt\n",
    "#!mv \"/content/Q2_2024/Lab_sessions/lab_session_01/data\" /content/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Application: liveability in the Netherlands` <br>\n",
    "In this lab session, we will explore how liveable the Netherlands is. Liveability is high on the policy agenda. Many policies directly or indirectly aim to improve liveability, such as e.g. lowering traffic speed, curbing noise levels, maintaining public gardens and spaces, etc. But liveability is hard to study due to its elusive and complex subjective nature. In this lab session, we will discover how liveability is spatially distributed, and what factors associate positively or negatively with liveability.<br>\n",
    "\n",
    "**Learning objectives**. After completing the following exercises you will be able to: <br>\n",
    "1. Construct a data set from multiple sources using `pandas`<br>\n",
    "2. Discover and visualise data, using `seaborn` and `geopandas`<br>\n",
    "3. Explore associations using scatter plots and correlation heat maps<br>\n",
    "4. Conduct linear regression, using `sk-learn`<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required Python packages and modules\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Import selected functions and classes from Python packages\n",
    "from os import getcwd\n",
    "from pathlib import Path\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Constructing a data set from multiple data sources<br>\n",
    "To construct a data set from multiple data sources, we will:<br>\n",
    "- A. **Load** the 3 data sets:<br>\n",
    "    - A.1. Liveability data<br>\n",
    "    - A.2. Population statistics data<br>\n",
    "    - A.3. geospatial data<br>\n",
    "- B. **Clean** the data<br>\n",
    "- C. **Combine** the data<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A. Loading data <br>\n",
    "\n",
    "We first load our primary data source on **liveability**. There are three liveability data sets, with varying **spatial levels**: <br>\n",
    "* `Gemeente`  Municipality \n",
    "* `Wijk`      Neighbourhood\n",
    "* `Buurt`     Local neighbourhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "working_folder = getcwd()\n",
    "data_folder = Path(f'data')\n",
    "print(data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load liveability data, using pandas\n",
    "gemeente_liv_data = pd.read_csv(data_folder/'gemeente_liveability.csv')\n",
    "wijk_liv_data = pd.read_csv(data_folder/'wijk_liveability.csv')\n",
    "buurt_liv_data = pd.read_csv(data_folder/'buurt_liveability.csv')\n",
    "print(list(gemeente_liv_data.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Description of variables*<br>\n",
    "`gm_code`&nbsp;&nbsp;&nbsp;Unique code for gemeente <br>\n",
    "`versie `&nbsp;&nbsp;&nbsp;Version of the 'Leefbaarometer model' used <br>\n",
    "`jaar   `&nbsp;&nbsp;&nbsp;&nbsp;&emsp;Year <br>\n",
    "`gm_naam`&emsp;Name of gemeente <br>\n",
    "\n",
    "*Liveability indicators:* <br>\n",
    "`lbm`   &emsp;**lbm** is the **key variable of interest** in this lab session. It is composed of the variables below: <br>\n",
    "`afw`   &emsp;Deviation from previous model <br>\n",
    "`fys`   &emsp;Physical environment<br>\n",
    "`onv`   &emsp;Nuisance and insecurity<br>\n",
    "`soc`   &emsp;Social cohesion<br>\n",
    "`vrz`   &emsp;Amenities<br>\n",
    "`won`   &emsp;Housing stock<br>\n",
    "\n",
    "We load the second source of data. These data contains population statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files on population statistics\n",
    "gemeente_pop_data = pd.read_csv(data_folder/'gemeente_2020_pop.csv', sep=';')\n",
    "wijk_pop_data = pd.read_csv(data_folder/'wijk_2020_pop.csv', sep=';')\n",
    "buurt_pop_data = pd.read_csv(data_folder/'buurt_2020_pop.csv', sep=';')\n",
    "gemeente_pop_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we load data on the geospatial data of gemeenten, wijken and buurten. To do this, load the **shape files** using geopandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load shape files\n",
    "gemeente_shape = gpd.read_file(data_folder/'gemeente/gemeente 2020.shp')\n",
    "wijk_shape = gpd.read_file(data_folder/'wijk/wijk 2020.shp')\n",
    "buurt_shape = gpd.read_file(data_folder/'buurt/buurt 2020.shp')\n",
    "print(buurt_shape.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the shape files are all in the right projection\n",
    "gemeente_shape.to_crs(epsg=28992, inplace=True)\n",
    "wijk_shape.to_crs(epsg=28992, inplace=True)\n",
    "buurt_shape.to_crs(epsg=28992, inplace=True)\n",
    "\n",
    "# Print crs info\n",
    "print(gemeente_shape.crs)\n",
    "print(wijk_shape.crs)\n",
    "print(buurt_shape.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### B. Cleaning data\n",
    "First, we clean the liveability data sets. <br>\n",
    "More specifically, we: <br>\n",
    "* Extract only those data points for which we have a liveability score.\n",
    "* Extract only those data points that are created by `Leefbaarometer 3.0` and for year `2020`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where lbm is NaN\n",
    "gemeente_liv_data.dropna(subset = ['lbm'], inplace = True)\n",
    "\n",
    "# Extract subsets: only year 2020 and Leefbaarometer 3.0 data\n",
    "gemeente_liv_data = gemeente_liv_data.loc[(gemeente_liv_data.versie ==  'Leefbaarometer 3.0') & (gemeente_liv_data.jaar == 2020)]\n",
    "wijk_liv_data.dropna(subset = ['lbm'], inplace = True)\n",
    "wijk_liv_data = wijk_liv_data.loc[(wijk_liv_data.versie ==  'Leefbaarometer 3.0') & (wijk_liv_data.jaar == 2020)]\n",
    "buurt_liv_data.dropna(subset = ['lbm'], inplace = True)\n",
    "buurt_liv_data = buurt_liv_data.loc[(buurt_liv_data.versie ==  'Leefbaarometer 3.0') & (buurt_liv_data.jaar == 2020)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we clean the population data sets.<br>\n",
    "More specifically, we: <br>\n",
    "* Remove useless data points (i.e. water areas, opposed to land areas)\n",
    "* Drop data points where the percentage of owner occupied homes and rented homes equals 0 (for later analyses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_pop_data(pop_data):\n",
    "\n",
    "    # Extract subsets: only data where is land\n",
    "    pop_data  = pop_data.loc[pop_data.H2O == 'NEE'] \n",
    "\n",
    "    # Remove rows where P_KOOPWON == 0 or P_HUURWON == 0\n",
    "    pop_data = pop_data.loc[(pop_data.P_KOOPWON>0) | (pop_data.P_HUURWON>0)]\n",
    "\n",
    "    return pop_data\n",
    "\n",
    "gemeente_pop_data = clean_pop_data(gemeente_pop_data)\n",
    "wijk_pop_data = clean_pop_data(wijk_pop_data)\n",
    "buurt_pop_data = clean_pop_data(buurt_pop_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### C. Combining data\n",
    "Now, we pull together the population and liveability data sets, using pandas' `merge` function. We use `GM_CODE`, `WK_CODE` and `BU_CODE` as key indices to merge on.<br>\n",
    "For the `merge` operation, the keys must be the same. Therefore, we first capitalise the keys of the shape files to be the same as in the other files.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capitalise column name of variables to merge on (to be the same across data sets)\n",
    "gemeente_liv_data.rename(columns={\"gm_code\": \"GM_CODE\"}, inplace=True)\n",
    "wijk_liv_data.rename(columns={\"wk_code\": \"WK_CODE\"}, inplace=True)\n",
    "buurt_liv_data.rename(columns={\"bu_code\": \"BU_CODE\"}, inplace=True)\n",
    "\n",
    "# Merge DataFrames\n",
    "gemeente_data = gemeente_pop_data.merge(gemeente_liv_data, on =\"GM_CODE\", how = \"inner\")\n",
    "wijk_data = wijk_pop_data.merge(wijk_liv_data, on=\"WK_CODE\", how = \"inner\")\n",
    "buurt_data = buurt_pop_data.merge(buurt_liv_data, on=\"BU_CODE\", how = \"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we add the geospatial (shape) data, using `merge`. Again, we use `GM_CODE`, `WK_CODE`, and `BU_CODE` as indices.<br>\n",
    "For the `merge` operation, the keys must be the same. Therefore, we first capitalise the keys of the shape files.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capitalise column name of key variables\n",
    "gemeente_shape = gemeente_shape.rename(columns={\"gm_code\": \"GM_CODE\"}).drop(columns = 'gm_naam')\n",
    "wijk_shape = wijk_shape.rename(columns={\"wk_code\": \"WK_CODE\"}).drop(columns = 'wk_naam')\n",
    "buurt_shape = buurt_shape.rename(columns={\"bu_code\": \"BU_CODE\"}).drop(columns = 'bu_naam')\n",
    "\n",
    "# Merge the GeoDataFrame and the DataFrame\n",
    "gemeente_df = gemeente_shape.merge(gemeente_data, on=\"GM_CODE\")\n",
    "wijk_df = wijk_shape.merge(wijk_data, on=\"WK_CODE\")\n",
    "buurt_df = buurt_shape.merge(buurt_data, on=\"BU_CODE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:skyblue\">Exercise 1: Exploring the data</span> \n",
    "\n",
    "`A` Load data file `Gemeenten alfabetisch 2020.excel` into a pandas DataFrame, using `pd.read_excel()`.<br>\n",
    "\n",
    "`B` Explore the content, e.g. using `.describe()`, `.head()`, and `.info()`\n",
    "<br>\n",
    "\n",
    "`C` Add the provincies (i.e. the column `Provincienaam`) of the Netherlands to the `gemeente_df` DataFrame, the `wijk_df` DataFrame. and the `buurt_df` DataFrame.<br>\n",
    "To do so, you need to merge two data set, based on the `GM_CODE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE YOUR ANSWERS HERE (Use as many cells as you need)\n",
    "# A\n",
    "gemeente2provincie_mapping2020 = pd.read_excel(data_folder/'gemeente2provincie_mapping2020.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B\n",
    "gemeente2provincie_mapping2020.describe()\n",
    "gemeente2provincie_mapping2020.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C\n",
    "gemeente_df = gemeente_df.merge(gemeente2provincie_mapping2020,on='GM_CODE')\n",
    "wijk_df = wijk_df.merge(gemeente2provincie_mapping2020,on='GM_CODE')\n",
    "buurt_df = buurt_df.merge(gemeente2provincie_mapping2020,on='GM_CODE')\n",
    "buurt_df['Provincienaam'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Discovering and visualising the data\n",
    "After having constructed our data set, it is time to explore the data more in depth. To do so, we use pandas `describe`, `head` and `info` functions. Additionally, to visualise the data, we create histograms (empirical cumulative density function plots). This is done using **seaborn** package. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first look at the key variable of interest: `lbm` (liveability), and visualise its ditribution using a histogram and a cumulative density function plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histogram and empirical CDF for variable(s) of interest\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5), sharex=True)\n",
    "sns.histplot(ax = axes[0],x = wijk_df.lbm)\n",
    "ecdf_data = sns.ecdfplot(ax = axes[1],x = wijk_df.lbm)\n",
    "axes[0].set_xlabel(\"lbm score\")\n",
    "axes[1].set_xlabel(\"lbm score\")\n",
    "axes[1].grid(True,linewidth = 0.5)\n",
    "axes[1].minorticks_on()\n",
    "axes[1].grid(which='minor', linestyle=':', linewidth='0.5', color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation of results**\n",
    "* Liveability of wijken is approximately normally distributed (no skew / higher moments)\n",
    "* The mean and median liveability of wijken is ~4.15\n",
    "* The domain of liveability is ~ [3.7-4.6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:skyblue\">Exercise 2: Distribution of social cohesion</span> \n",
    "Besides liveability, in the liveability data there is also a variable on social cohesion `soc`.<br>\n",
    "`A` Conduct several descriptive analyses, such as histograms, boxplots, etc, on the variable `soc`<br>\n",
    "`B` Interpret your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE YOUR ANSWERS HERE (Use as many cells as you need)\n",
    "# A\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5), sharex=True)\n",
    "sns.histplot(ax = axes[0],x = wijk_df.soc)\n",
    "ecdf_data = sns.ecdfplot(ax = axes[1],x = wijk_df.soc)\n",
    "axes[0].set_xlabel(\"social cohesion\")\n",
    "axes[1].set_xlabel(\"social cohesion\")\n",
    "axes[1].grid(True,linewidth = 0.5)\n",
    "axes[1].minorticks_on()\n",
    "axes[1].grid(which='minor', linestyle=':', linewidth='0.5', color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B\n",
    "# There seems to be a large spread. Furthermore, we see that this variable is 0 centrered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important step when discovering data is to evaluate their **face validity**. To do so, let's see whether to top-rated wijken seems to make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List top-5 best wijken, based on liveability\n",
    "wijk_df.nlargest(5,'lbm').head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:skyblue\">Exercise 3: Face validity of worst neighbourhoods (wijken)</span> \n",
    "The best scoring neighbourhoods seem to make sense. Let's also look at the worst scoring neighbourhoods.<br>\n",
    "`A` List the 10 neighourhoods (wijken) which score lowest on `lbm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE YOUR ANSWERS HERE (Use as many cells as you need)\n",
    "# A\n",
    "wijk_df.nsmallest(5,'lbm').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have added geospatial data to the dataframe, we can visualise the data in space. <br> \n",
    "To further discover the data, let's first look at the municipality level. We color municipalities based on the average liveability score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the liveability in the Netherlands at the wijk-level\n",
    "fig, ax = plt.subplots(figsize=(60,60))\n",
    "wijk_df.plot(ax=ax, column = 'lbm', legend = True)\n",
    "gemeente_df.plot(ax=ax, color  = 'none', edgecolor='black')\n",
    "_=gemeente_df.apply(lambda x: ax.annotate(text=x['GM_NAAM'], xy=x.geometry.centroid.coords[0], ha='center'), axis=1)\n",
    "# gemeente_df.apply(lambda x: '' if x['AANT_INW']<50000 else ax.annotate(text=x['GM_NAAM'], xy=x.geometry.centroid.coords[0], ha='center'), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation of results**\n",
    "* Wijken located near the coast (including the islands) score above average.\n",
    "* Larger cities (e.g. AMS, RT, Tilburg, Utrecht, The Hague) seem to have multiple very poor performing wijken\n",
    "* In some municipalities, data are patchy (missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we do the same, but at the lowest spatial level (buurten) focusing on the municipality of Delft. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the liveability in Delft at the buurt-level\n",
    "fig, ax = plt.subplots(figsize=(40,40))\n",
    "buurt_df.loc[buurt_df.GM_NAAM == 'Delft'].plot(ax=ax, column = 'lbm', legend = True)\n",
    "gemeente_df.loc[gemeente_df.GM_NAAM == 'Delft'].plot(ax=ax, color  = 'none', edgecolor='black')\n",
    "buurt_df.loc[buurt_df.GM_NAAM == 'Delft'].apply(lambda x: ax.annotate(text=x['BU_NAAM'], xy=x.geometry.centroid.coords[0], ha='center'), axis=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:skyblue\">Exercise 4: Visualise local neighbourhoods in Zuid-Holland</span> \n",
    "`A` Plot the liveability score in Zuid-Holland at the buurt level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE YOUR ANSWERS HERE (Use as many cells as you need)\n",
    "# A\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(40,40))\n",
    "buurt_df.loc[buurt_df.Provincienaam == 'Zuid-Holland'].plot(ax=ax, column = 'lbm', legend = True)\n",
    "gemeente_df.loc[gemeente_df.Provincienaam == 'Zuid-Holland'].plot(ax=ax, color  = 'none', edgecolor='black')\n",
    "gemeente_df.loc[gemeente_df.Provincienaam == 'Zuid-Holland'].apply(lambda x: ax.annotate(text=x['GM_NAAM'], xy=x.geometry.centroid.coords[0], ha='center'), axis=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Exploring associations\n",
    "To explore associations, we use correlation heat maps, scatter plots and boxplots <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap of correlations\n",
    "# Create plot\n",
    "fig, axes = plt.subplots(figsize=(10, 10))\n",
    "fig.set_tight_layout(True)\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr = wijk_df[['BEV_DICHTH','AANT_INW','AF_SUPERM','AF_RESTAU','P_KOOPWON','P_HUURWON', 'fys', 'onv', 'soc', 'vrz', 'won','lbm']].corr()\n",
    "\n",
    "# Create upper triangular matrix to mask the upper triangular part of the heatmap\n",
    "corr_mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Generate a custom diverging colormap (because it looks better)\n",
    "corr_cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "sns.heatmap(corr, mask = corr_mask, cmap=corr_cmap, annot=True,square = True, linewidths=.5, ax = axes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation of the results** <br>\n",
    "1. Pearson corr coefficients range from [-0.99,0.86] \n",
    "2. *P_HUURWON* and *P_KOOPWON* correlate (almost) 1 to 1 (which makes sense).<br>\n",
    "\n",
    "Looking at the bottom row for lbm, we see that:<br>\n",
    "\n",
    "3. *BEV_DICHTH*, *AANT_INW*, *P_HUURWON* correlate negatively with liveability\n",
    "4. *AF_SUPERM*, *AF_RESTAU*, *P_KOOPWON*, *fys*, *onv*, *soc*, *vrz*, *won* correlate positively with liveability\n",
    "5. lbm correlates strongest with *onv* (nuisance and insecurity), *soc* (social cohesion), *won* (housing stock)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further explore linear associations using scatter plots\n",
    "# plot\n",
    "fig, axes = plt.subplots(3, 2, figsize=(20, 10))\n",
    "fig.set_tight_layout(True)\n",
    "sns.scatterplot(ax = axes[0,0],x = wijk_df.BEV_DICHTH, y = wijk_df.lbm, alpha = 0.2)\n",
    "sns.scatterplot(ax = axes[0,1],x = wijk_df.AANT_INW, y = wijk_df.lbm, alpha = 0.2)   \n",
    "sns.scatterplot(ax = axes[1,0],x = wijk_df.AF_SUPERM, y = wijk_df.lbm, alpha = 0.2)\n",
    "sns.scatterplot(ax = axes[1,1],x = wijk_df.AF_RESTAU, y = wijk_df.lbm, alpha = 0.2)\n",
    "sns.scatterplot(ax = axes[2,0],x = wijk_df.P_KOOPWON[wijk_df.P_KOOPWON>0], y = wijk_df.lbm[wijk_df.P_KOOPWON>0], alpha = 0.2)   \n",
    "sns.scatterplot(ax = axes[2,1],x = wijk_df.P_HUURWON[wijk_df.P_HUURWON>0], y = wijk_df.lbm[wijk_df.P_HUURWON>0], alpha = 0.2)   \n",
    "\n",
    "\n",
    "axes[0,0].set_xlabel('Population density [#/km^2]')\n",
    "axes[0,1].set_xlabel('Population size [#]')\n",
    "axes[1,0].set_xlabel('Distance to supermarket [km]')\n",
    "axes[1,1].set_xlabel('Distance to restaurant [km]')\n",
    "axes[2,0].set_xlabel('Percentage owner occupied homes [%]')\n",
    "axes[2,1].set_xlabel('Percentage rental homes [%]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation of the results** <br>\n",
    "* The scatter plots show the bivariate relations with liveability is strongest for *P_KOOPWON* and *P_HUURWON*.\n",
    "* For *BEV_DICHTH*, *AANT_INW*, *AF_RESTAU* and *P_KOOPWON* the data seems skewed, condensed around x = 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Boxplot analysis** <br>\n",
    "\n",
    "When one of the features is categorical a **boxplot** can be insightful. <br>\n",
    "Boxplots are informative as they also reveal insights on the locality, spread and skewness of numerical data through their quartiles, see https://en.wikipedia.org/wiki/Box_plot.<br>\n",
    "Below we create a boxplot showing the liveability across a selected number of municipalities based on the liveabilities scores at the buurt level.\n",
    "Add your own municipality to see if the liveability scores are in line with your own perceptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot\n",
    "fig, axes = plt.subplots(1, 1, figsize=(15, 10), sharex=True)\n",
    "GM_select = ['Amsterdam','Rotterdam','Utrecht',\"'s-Gravenhage\",'Groningen','Tilburg']\n",
    "df_GM_select = buurt_df[buurt_df['GM_NAAM'].isin(GM_select)]\n",
    "sns.boxplot(ax = axes, y = df_GM_select.lbm, x = df_GM_select.GM_NAAM, orient=\"v\" )\n",
    "\n",
    "axes.grid(True,linewidth = 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation of results**\n",
    "* Looking at the **median** scores, Amsterdam and Groninger are the best municipalities to live in. <br>\n",
    "* Unlike Groningen, Amsterdam also has very unliveable buurten. This can be seen from the outliers at the lower side.\n",
    "* Larger cities seem to have a higher variance in the liveability scores across buurten (which is to be expected).\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:skyblue\">Exercise 5:  Scatter the median liveability scores of gemeentes based on wijk and buurt level data</span> \n",
    "`A` Create a scatter plot in which the median liveability score of the gemeente as computed using wijk level data is at the x-axis and the median liveability score as computed using buurt level data is at the y-axis. (hint: use the pandas method groupby)<br>\n",
    "`B` Interpret the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE YOUR ANSWERS HERE (Use as many cells as you need)\n",
    "# A\n",
    "# Get median values per gemeente\n",
    "median_lbm_on_wk = wijk_df.groupby('GM_CODE')['lbm'].median()\n",
    "median_lbm_on_bu = buurt_df.groupby('GM_CODE')['lbm'].median()\n",
    "\n",
    "fig, axes = plt.subplots(1, 1, figsize=(25, 25))\n",
    "sns.scatterplot(x = median_lbm_on_wk, y = median_lbm_on_bu)\n",
    "axes.set_aspect('equal', adjustable='box')\n",
    "axes.grid(True,linewidth = 0.5)\n",
    "\n",
    "# Add diagonal line\n",
    "x_diag = [min(median_lbm_on_bu),max(median_lbm_on_bu)]\n",
    "y_diag = [min(median_lbm_on_bu),max(median_lbm_on_bu)]\n",
    "sns.lineplot(x = x_diag, y = x_diag,linestyle = ':',color = 'k')\n",
    "\n",
    "# Loop through the data points \n",
    "for i, gem_name in enumerate(gemeente_df.GM_CODE):\n",
    "    plt.text(median_lbm_on_wk.iloc[i],median_lbm_on_bu.iloc[i],gemeente_df.GM_NAAM.iloc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conducting multiple linear regression analyses\n",
    "Regression models are often used to obtain first insights on the relationship between a (scalar) target and one or more features. Regression models come in many different forms: e.g. simple, multiple, ordinal, Poisson, etc. Usually, a multiple linear regression model serves as a good benchmark.\n",
    "\n",
    "In ML the focus is usually on generalisation. That is, the model performance is evaluated by looking at the performance on an unseen set of the data (generalisation performance).<br>\n",
    "\n",
    "For these analyses, we use the **sk-learn** Python library. This Python library is especially designed machine learning.\n",
    "Having seen the scatter plots and bar plots, we focus on the following 7 features: *ANNT_INW*, *BEV_DICHTH*, *P_KOOPWON*, *fys*, *soc*, *won*, and *onv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed number for reproducibility. It governs which data points go to the test set and which go to the training set\n",
    "rs = 12345\n",
    "\n",
    "# Create X\n",
    "X_lin = pd.DataFrame([buurt_df[\"AANT_INW\"].div(100000),buurt_df[\"BEV_DICHTH\"].div(10000), buurt_df[\"P_KOOPWON\"].div(100),buurt_df[\"fys\"],buurt_df[\"soc\"],buurt_df[\"won\"],buurt_df[\"onv\"]]).transpose()\n",
    "\n",
    "\n",
    "# Create Y\n",
    "Y = buurt_df.lbm\n",
    "\n",
    "# Create linear regression object\n",
    "regr = LinearRegression(fit_intercept = True)\n",
    "\n",
    "# Split the data in a train and test set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_lin, Y, test_size=0.25,random_state = rs)\n",
    "\n",
    "# Fit the model on the training data\n",
    "regr.fit(X_train,Y_train)\n",
    "\n",
    "# Evaluate the model generalisation performance on the Train and Test data sets\n",
    "Y_pred_train = regr.predict(X_train)\n",
    "mse_train = mean_squared_error(Y_train,Y_pred_train)\n",
    "R2_train = r2_score(Y_train,Y_pred_train)\n",
    "\n",
    "Y_pred_test = regr.predict(X_test)\n",
    "mse_test  = mean_squared_error(Y_test, Y_pred_test)\n",
    "R2_test = r2_score(Y_test,Y_pred_test)\n",
    "\n",
    "# Print results\n",
    "print('Results linear multiple regression model')\n",
    "print(f'Mean Squared Error Train | Test: \\t{mse_train:>7.4f}\\t|  {mse_test:>7.4f}')\n",
    "print(f'R2                 Train | Test: \\t{R2_train:>7.4f}\\t|  {R2_test:>7.4f}\\n')\n",
    "print('Weights')\n",
    "print(f'Intercept: \\t\\t\\t {regr.intercept_:>7.4f}')\n",
    "for n in range(len(regr.coef_)):\n",
    "    print(f'Weight_{X_lin.keys()[n]:10s} \\t\\t {regr.coef_[n]:>7.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation of results**\n",
    "* Looking at the MSE and R2, we see that the performance on the **test** data set is roughly equal to the performance on the **train** data set. This tells us that the model is not overfitting.\n",
    "* An advantage of regression models is that their weights are interpretable. For instance, the positive and comparatively large weight associated with *BEV_DICHTH* tells us that high population density is associated with high liveability. \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:skyblue\">Exercise 6:  Impact of seed number on the regression results</span> \n",
    "`A` Re-do the regression using different seed numbers (e.g. 1,2,3,4. etc) and reflect on the stability of the results<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE YOUR ANSWERS HERE (Use as many cells as you need)\n",
    "# A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:skyblue\">Exercise 7:  Generalisation</span> \n",
    "`A` Train a new extended multiple regression model. To do so, add 13 extra features to the model that could be associated with liveability. For this extended model, compute the Mean Squared Error and R2 both for the train and test data sets.<br>\n",
    "`B` The picture below shows the Bias-Variance trade-off. Looking at the MSE and R2 of the model with 7 features and the extended model (with 20 features), where do you think these two models can be placed in the picture (Region A, B, C, D, or E)?  Explain your answer.\n",
    "<br><br><br>\n",
    "![Bias_variance_tradeoff](data/Bias_variance.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE YOUR ANSWERS HERE (Use as many cells as you need)\n",
    "# A\n",
    "# Your code here\n",
    "# Use as many cells as you need\n",
    "\n",
    "# A\n",
    "\n",
    "# Create data frames\n",
    "frames1 = pd.DataFrame([buurt_df[\"AANT_INW\"].div(100000),buurt_df[\"BEV_DICHTH\"].div(10000), buurt_df[\"P_KOOPWON\"].div(100),buurt_df[\"fys\"],buurt_df[\"soc\"],buurt_df[\"won\"],buurt_df[\"onv\"]]).transpose()\n",
    "frames2 = buurt_df[['P_15_24_JR','P_STERFT','P_GESCHEID','STED','WOZ','P_MAROKKO','P_ANT_ARU','P_SURINAM','P_TURKIJE','P_1GEZW','P_MGEZW','OAD','AANT_VROUW']]\n",
    "\n",
    "X_lin_extended = pd.concat([frames1,frames2],axis=1)\n",
    "X_lin_extended.replace(-99999999,np.nan, inplace = True)\n",
    "X_lin_extended.fillna(X_lin_extended.mean(),inplace = True)\n",
    "\n",
    "# Create Y\n",
    "Y = buurt_df.lbm\n",
    "\n",
    "# Create linear regression object\n",
    "regr = LinearRegression(fit_intercept = True)\n",
    "\n",
    "# Split the data in a train and test set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_lin_extended, Y, test_size=0.25,random_state = rs)\n",
    "\n",
    "# Fit the model on the training data\n",
    "regr.fit(X_train,Y_train)\n",
    "\n",
    "# Evaluate the model generalisation performance on the Train and Test data sets\n",
    "Y_pred_train = regr.predict(X_train)\n",
    "mse_train = mean_squared_error(Y_train,Y_pred_train)\n",
    "R2_train = r2_score(Y_train,Y_pred_train)\n",
    "\n",
    "Y_pred_test = regr.predict(X_test)\n",
    "mse_test  = mean_squared_error(Y_test, Y_pred_test)\n",
    "R2_test = r2_score(Y_test,Y_pred_test)\n",
    "\n",
    "# Print results\n",
    "print('Results linear multiple regression model')\n",
    "print(f'Mean Squared Error Train | Test: \\t{mse_train:>7.4f}\\t|  {mse_test:>7.4f}')\n",
    "print(f'R2                 Train | Test: \\t{R2_train:>7.4f}\\t|  {R2_test:>7.4f}\\n')\n",
    "print('Weights')\n",
    "print(f'Intercept: \\t\\t\\t {regr.intercept_:>7.4f}')\n",
    "for n in range(len(regr.coef_)):\n",
    "    print(f'Weight_{X_lin_extended.keys()[n]:10s} \\t\\t {regr.coef_[n]:>7.4f}')\n",
    "\n",
    "# B \n",
    "# The extended model is probably in region B. We conclude this because we still see little overfitting going on. This suggests the model could further increase in complexity. \n",
    "# As a result, we can conclude the first model is probably in Region A. It fits worse that the extended model. This says that it is more biased."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testerfinal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
